{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install emot"
      ],
      "metadata": {
        "id": "x-TeZxiyQQpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.svm import SVR\n",
        "from scipy.stats import pearsonr\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from emot.emo_unicode import UNICODE_EMOJI\n",
        "\n",
        "\n",
        "dataset = pd. read_csv(\"fear-ratings-0to1.train.txt\", sep=\"\\t\", names=['id', 'tweet', 'emotion', 'score'])\n",
        "dataset1 = pd. read_csv(\"fear-ratings-0to1.dev.gold.txt\", sep=\"\\t\", names=['id', 'tweet', 'emotion', 'score'])\n",
        "dataset2 = pd. read_csv(\"fear-ratings-0to1.test.gold.txt\", sep=\"\\t\", names=['id', 'tweet', 'emotion', 'score'])\n",
        "test = pd.read_csv(\"fear-pred.txt\", sep = \"\\t\", names = ['id', 'tweet', 'emotion', 'score'])\n",
        "train = pd.concat([dataset, dataset1,dataset2], ignore_index = True)\n",
        "\n",
        "X = train.tweet\n",
        "y = train.score\n",
        "X_test = test.tweet\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def convert_emojis(text):\n",
        "    for emot in UNICODE_EMOJI:\n",
        "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
        "    return text\n",
        "# Define the preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags and special characters\n",
        "    text = convert_emojis(text)\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
        "    text = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\",text)\n",
        "    # FLAGS = re.MULTILINE | re.DOTALL\n",
        "    eyes = r\"[8:=;]\"\n",
        "    nose = r\"['`\\-]?\"\n",
        "    text = re.sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\",text)\n",
        "    text = re.sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\",text)\n",
        "    text = re.sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\",text)\n",
        "    text = re.sub(r\"<3\",\"<heart>\",text)\n",
        "    text = re.sub(r\"#\\S+\", lambda hashtag: \" \".join(segment(hashtag.group()[1:])),text)\n",
        "\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    # Join the tokens back into a string\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocess', FunctionTransformer(lambda x: [preprocess_text(text) for text in x], validate=False)),\n",
        "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
        "    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X, y)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "predictions = pipeline.predict(X_test)\n",
        "test['score'] = predictions\n",
        "\n",
        "test.to_csv('fear-prediction.txt', index = False)"
      ],
      "metadata": {
        "id": "G6EFsnoW9L-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model using Deep Learning"
      ],
      "metadata": {
        "id": "q8n4tT4xRPHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "data = pd.read_csv(\"fear-ratings-0to1.train.txt\", sep=\"\\t\", names=['id', 'tweet', 'emotion', 'score'])\n",
        "\n",
        "# Preprocess the data\n",
        "X = data['tweet'].values\n",
        "y = data['score'].values\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokens = tokenizer(X.tolist(), padding=True, truncation=True, return_tensors='tf', max_length=128)\n",
        "\n",
        "# Load BERT model\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Extract BERT embeddings\n",
        "embeddings = bert_model(tokens.input_ids).last_hidden_state[:, 0, :]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_embeddings, X_test_embeddings, y_train, y_test = train_test_split(embeddings.numpy(), y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build a simple feedforward neural network with regression head\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(768,)),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_embeddings, y_train, epochs=100, batch_size=32, validation_data=(X_test_embeddings, y_test))\n",
        "\n",
        "# Calculate overall correlation on the entire dataset\n",
        "y_pred_all = np.squeeze(model.predict(embeddings.numpy()))\n",
        "overall_pearson_corr, _ = pearsonr(y, y_pred_all)\n",
        "overall_spearman_corr, _ = spearmanr(y, y_pred_all)\n",
        "\n",
        "print(f'Overall Pearson Correlation: {overall_pearson_corr:.4f}, Overall Spearman Correlation: {overall_spearman_corr:.4f}')\n"
      ],
      "metadata": {
        "id": "3BB2tgY6vSuu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
